{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOfq2JK_xLa6"
      },
      "source": [
        "# File-level and Chunk-Level Retrieval with LlamaCloud and Workflows\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/run-llama/llamacloud-demo/blob/main/examples/advanced_rag/file_retrieve_workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "In this notebook we will show you how to perform file-level and chunk-level retrieval with LlamaCloud using a custom router query engine and a custom agent built with [Workflows](https://docs.llamaindex.ai/en/latest/module_guides/workflow/).\n",
        "\n",
        "![](https://github.com/run-llama/llamacloud-demo/blob/main/examples/advanced_rag/file_retrieve_workflow_img.png?raw=1)\n",
        "\n",
        "File-level retrieval is useful for handling user questions that require the entire document context to properly answer the question. Since only doing file-level retrieval can be slow + expensive, we also show you how to build an agent that can dynamically decide whether to do file-level or chunk-level retrieval!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs27fe0BxLa7"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Install LlamaIndex, apply nest_asyncio, and set up your OpenAI API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mbUFKt0kxLa7",
        "outputId": "7b528079-05f5-4b59-a4ba-f2af7e2a41da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.11.20-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index-agent-openai<0.4.0,>=0.3.4 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.3.4-py3-none-any.whl.metadata (728 bytes)\n",
            "Collecting llama-index-cli<0.4.0,>=0.3.1 (from llama-index)\n",
            "  Downloading llama_index_cli-0.3.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.12.0,>=0.11.20 (from llama-index)\n",
            "  Downloading llama_index_core-0.11.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting llama-index-embeddings-openai<0.3.0,>=0.2.4 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl.metadata (686 bytes)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting llama-index-llms-openai<0.3.0,>=0.2.10 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.2.16-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.2.3-py3-none-any.whl.metadata (729 bytes)\n",
            "Collecting llama-index-program-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\n",
            "Collecting llama-index-readers-file<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.2.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting nltk>3.8.1 (from llama-index)\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud)\n",
            "  Downloading llama_cloud-0.1.4-py3-none-any.whl.metadata (814 bytes)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud) (0.27.2)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud) (2.9.2)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.52.2)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.20->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.20->llama-index) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.20->llama-index) (3.10.10)\n",
            "Collecting dataclasses-json (from llama-index-core<0.12.0,>=0.11.20->llama-index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.20->llama-index) (1.2.14)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.12.0,>=0.11.20->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.20->llama-index) (2024.10.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.20->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.20->llama-index) (3.4.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.20->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.20->llama-index) (10.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.20->llama-index) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.12.0,>=0.11.20->llama-index)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.12.0,>=0.11.20->llama-index)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.20->llama-index) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.20->llama-index) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.12.0,>=0.11.20->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.20->llama-index) (1.16.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.12.3)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index)\n",
            "  Downloading llama_parse-0.5.12-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (2024.9.11)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.20->llama-index) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.20->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.20->llama-index) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.20->llama-index) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.20->llama-index) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.20->llama-index) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.20->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.20.0->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud) (0.14.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (0.6.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.20->llama-index) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.20->llama-index) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.20->llama-index) (3.1.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.20->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.12.0,>=0.11.20->llama-index)\n",
            "  Downloading marshmallow-3.23.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.20.0->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud) (1.2.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.20->llama-index) (24.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.20->llama-index) (0.2.0)\n",
            "Downloading llama_index-0.11.20-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl (10 kB)\n",
            "Downloading llama_cloud-0.1.4-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.8/176.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_agent_openai-0.3.4-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.3.1-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_core-0.11.21-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.2.16-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.2.3-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.2.2-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading llama_parse-0.5.12-py3-none-any.whl (13 kB)\n",
            "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.23.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: striprtf, dirtyjson, tenacity, pypdf, nltk, mypy-extensions, marshmallow, typing-inspect, tiktoken, llama-cloud, dataclasses-json, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed dataclasses-json-0.6.7 dirtyjson-1.0.8 llama-cloud-0.1.4 llama-index-0.11.20 llama-index-agent-openai-0.3.4 llama-index-cli-0.3.1 llama-index-core-0.11.21 llama-index-embeddings-openai-0.2.5 llama-index-indices-managed-llama-cloud-0.4.0 llama-index-legacy-0.9.48.post3 llama-index-llms-openai-0.2.16 llama-index-multi-modal-llms-openai-0.2.3 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.2.2 llama-index-readers-llama-parse-0.3.0 llama-parse-0.5.12 marshmallow-3.23.0 mypy-extensions-1.0.0 nltk-3.9.1 pypdf-4.3.1 striprtf-0.0.26 tenacity-8.5.0 tiktoken-0.8.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "%pip install llama-index llama-index-indices-managed-llama-cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b8sj_5guxLa8"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU365IFGxLa8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<Your OpenAI API Key>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkVwl6pgxLa8"
      },
      "source": [
        "## Load Documents into LlamaCloud\n",
        "\n",
        "The first order of business is to download the 5 Apple and Tesla 10Ks and upload them into LlamaCloud.\n",
        "\n",
        "You can easily do this by creating a pipeline and uploading docs via the \"Files\" mode.\n",
        "\n",
        "After this is done, proceed to the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFzJGyfnxLa9"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data\n",
        "# download Apple\n",
        "!wget \"https://s2.q4cdn.com/470004039/files/doc_earnings/2023/q4/filing/_10-K-Q4-2023-As-Filed.pdf\" -O data/apple_2023.pdf\n",
        "!wget \"https://s2.q4cdn.com/470004039/files/doc_financials/2022/q4/_10-K-2022-(As-Filed).pdf\" -O data/apple_2022.pdf\n",
        "!wget \"https://s2.q4cdn.com/470004039/files/doc_financials/2021/q4/_10-K-2021-(As-Filed).pdf\" -O data/apple_2021.pdf\n",
        "!wget \"https://s2.q4cdn.com/470004039/files/doc_financials/2020/ar/_10-K-2020-(As-Filed).pdf\" -O data/apple_2020.pdf\n",
        "!wget \"https://www.dropbox.com/scl/fi/i6vk884ggtq382mu3whfz/apple_2019_10k.pdf?rlkey=eudxh3muxh7kop43ov4bgaj5i&dl=1\" -O data/apple_2019.pdf\n",
        "\n",
        "# download Tesla\n",
        "!wget \"https://ir.tesla.com/_flysystem/s3/sec/000162828024002390/tsla-20231231-gen.pdf\" -O data/tesla_2023.pdf\n",
        "!wget \"https://ir.tesla.com/_flysystem/s3/sec/000095017023001409/tsla-20221231-gen.pdf\" -O data/tesla_2022.pdf\n",
        "!wget \"https://www.dropbox.com/scl/fi/ptk83fmye7lqr7pz9r6dm/tesla_2021_10k.pdf?rlkey=24kxixeajbw9nru1sd6tg3bye&dl=1\" -O data/tesla_2021.pdf\n",
        "!wget \"https://ir.tesla.com/_flysystem/s3/sec/000156459021004599/tsla-10k_20201231-gen.pdf\" -O data/tesla_2020.pdf\n",
        "!wget \"https://ir.tesla.com/_flysystem/s3/sec/000156459020004475/tsla-10k_20191231-gen_0.pdf\" -O data/tesla_2019.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBKtZpCsxLa9"
      },
      "source": [
        "## Helper Classes\n",
        "\n",
        "We define the `Answer` model, which is a model that stores whether to pick chunk-level retrieval or document-level retrieval, along with a reason for that choice. We will let the LLM choose given a query string, and we will ask the LLM to produce a JSON output that can be parsed by a Pydantic model.\n",
        "\n",
        "We will define the `RouterOutputParser` helper class, which parses the output from the LLM into a list of `Answer` models, which is then put into the `Answers` model that contains a list of `Answer`s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PrYnpp58xLa9"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "from llama_index.core.bridge.pydantic import BaseModel\n",
        "from typing import List\n",
        "from llama_index.core.types import BaseOutputParser\n",
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "# tells LLM to select choices given a list\n",
        "ROUTER_PROMPT = PromptTemplate(\n",
        "    \"Some choices are given below. It is provided in a numbered list (1 to\"\n",
        "    \" {num_choices}), where each item in the list corresponds to a\"\n",
        "    \" summary.\\n---------------------\\n{context_list}\\n---------------------\\nUsing\"\n",
        "    \" only the choices above and not prior knowledge, return the top choices\"\n",
        "    \" (no more than {max_outputs}, but only select what is needed) that are\"\n",
        "    \" most relevant to the question: '{query_str}'\\n\"\n",
        ")\n",
        "\n",
        "# tells LLM to format list of choices in a certain way\n",
        "FORMAT_STR = \"\"\"The output should be formatted as a JSON instance that conforms to\n",
        "the JSON schema below.\n",
        "\n",
        "Here is the output schema:\n",
        "{\n",
        "  \"type\": \"array\",\n",
        "  \"items\": {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "      \"choice\": {\n",
        "        \"type\": \"integer\"\n",
        "      },\n",
        "      \"reason\": {\n",
        "        \"type\": \"string\"\n",
        "      }\n",
        "    },\n",
        "    \"required\": [\n",
        "      \"choice\",\n",
        "      \"reason\"\n",
        "    ],\n",
        "    \"additionalProperties\": false\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "class Answer(BaseModel):\n",
        "    \"\"\"Answer model.\"\"\"\n",
        "\n",
        "    choice: int\n",
        "    reason: str\n",
        "\n",
        "\n",
        "class Answers(BaseModel):\n",
        "    \"\"\"List of answers model.\"\"\"\n",
        "\n",
        "    answers: List[Answer]\n",
        "\n",
        "class RouterOutputParser(BaseOutputParser):\n",
        "    \"\"\"Custom output parser.\"\"\"\n",
        "\n",
        "    def _escape_curly_braces(self, input_string: str):\n",
        "        \"\"\"Escape the brackets in the format string so contents are not treated as variables.\"\"\"\n",
        "\n",
        "        return input_string.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
        "\n",
        "    def _marshal_output_to_json(self, output: str):\n",
        "        \"\"\"Find JSON string within response.\"\"\"\n",
        "\n",
        "        output = output.strip()\n",
        "        left = output.find(\"[\")\n",
        "        right = output.find(\"]\")\n",
        "        output = output[left : right + 1]\n",
        "        return output\n",
        "\n",
        "    def parse(self, output: str) -> Answers:\n",
        "        \"\"\"Parse string\"\"\"\n",
        "\n",
        "        json_output = self._marshal_output_to_json(output)\n",
        "        json_dicts = json.loads(json_output)\n",
        "        answers = [Answer.parse_obj(json_dict) for json_dict in json_dicts]\n",
        "        return Answers(answers=answers)\n",
        "\n",
        "    def format(self, query: str) -> str:\n",
        "        return query + \"\\n\\n\" + self._escape_curly_braces(FORMAT_STR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NJALHwSxLa-"
      },
      "source": [
        "## Router Query Workflow\n",
        "\n",
        "In the code snippet below, we define a router query workflow. This workflow requires 2 events: a `ChooseQueryEngineEvent`, which chooses the document-level or chunk-retrieval query engine, and `SynthesizeAnswersEvent`, which contains the results from the query engines and synthesizes a final response.\n",
        "\n",
        "The workflow consists of the following steps:\n",
        "1. Choosing the query engine(s) by passing the prompt and output parser defined above into an LLM. Both query engines can be chosen if the LLM thinks both query engines (defined in `choose_query_engine()`).\n",
        "2. Queries the engines chosen by the LLM in the previous step (defined in `query_each_engine`).\n",
        "3. Synthesizes a final response given the results from the queries above (defined in `synthesize_response()`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "olZb3G5GxLa-"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional, Any\n",
        "\n",
        "from llama_index.core.query_engine import (\n",
        "    BaseQueryEngine,\n",
        "    RetrieverQueryEngine,\n",
        ")\n",
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core.llms import LLM\n",
        "from llama_index.core.response_synthesizers import TreeSummarize\n",
        "from llama_index.core.workflow import (\n",
        "    Workflow,\n",
        "    Event,\n",
        "    StartEvent,\n",
        "    StopEvent,\n",
        "    step,\n",
        ")\n",
        "\n",
        "class ChooseQueryEngineEvent(Event):\n",
        "    \"\"\"Query engine event.\"\"\"\n",
        "\n",
        "    answers: Answers\n",
        "    query_str: str\n",
        "\n",
        "class SynthesizeAnswersEvent(Event):\n",
        "    \"\"\"Synthesize answers event.\"\"\"\n",
        "\n",
        "    responses: List[Any]\n",
        "    query_str: str\n",
        "\n",
        "\n",
        "class RouterQueryWorkflow(Workflow):\n",
        "    \"\"\"Router query workflow.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        query_engines: List[BaseQueryEngine],\n",
        "        choice_descriptions: List[str],\n",
        "        router_prompt: PromptTemplate,\n",
        "        timeout: Optional[float] = 10.0,\n",
        "        disable_validation: bool = False,\n",
        "        verbose: bool = False,\n",
        "        llm: Optional[LLM] = None,\n",
        "        summarizer: Optional[TreeSummarize] = None,\n",
        "    ):\n",
        "        \"\"\"Constructor\"\"\"\n",
        "\n",
        "        super().__init__(timeout=timeout, disable_validation=disable_validation, verbose=verbose)\n",
        "\n",
        "        self.query_engines: List[BaseQueryEngine] = query_engines\n",
        "        self.choice_descriptions: List[str] = choice_descriptions\n",
        "        self.router_prompt: PromptTemplate = router_prompt\n",
        "        self.llm: LLM = llm or OpenAI(temperature=0, model=\"gpt-4o\")\n",
        "        self.summarizer: TreeSummarize = summarizer or TreeSummarize()\n",
        "\n",
        "    def _get_choice_str(self, choices):\n",
        "        \"\"\"String of choices to feed into LLM.\"\"\"\n",
        "\n",
        "        choices_str = \"\\n\\n\".join([f\"{idx+1}. {c}\" for idx, c in enumerate(choices)])\n",
        "        return choices_str\n",
        "\n",
        "    async def _query(self, query_str: str, choice_idx: int):\n",
        "        \"\"\"Query using query engine\"\"\"\n",
        "\n",
        "        query_engine = self.query_engines[choice_idx]\n",
        "        return await query_engine.aquery(query_str)\n",
        "\n",
        "\n",
        "    @step()\n",
        "    async def choose_query_engine(self, ev: StartEvent) -> ChooseQueryEngineEvent:\n",
        "        \"\"\"Choose query engine.\"\"\"\n",
        "\n",
        "        # get query str\n",
        "        query_str = ev.get(\"query_str\")\n",
        "        if query_str is None:\n",
        "            raise ValueError(\"'query_str' is required.\")\n",
        "\n",
        "        # partially format prompt with number of choices and max outputs\n",
        "        router_prompt1 = self.router_prompt.partial_format(\n",
        "            num_choices=len(self.choice_descriptions),\n",
        "            max_outputs=len(self.choice_descriptions),\n",
        "        )\n",
        "\n",
        "\n",
        "        # get choices selected by LLM\n",
        "        choices_str = self._get_choice_str(self.choice_descriptions)\n",
        "        output = llm.structured_predict(\n",
        "            Answers,\n",
        "            router_prompt1,\n",
        "            context_list=choices_str,\n",
        "            query_str=query_str\n",
        "        )\n",
        "\n",
        "        if self._verbose:\n",
        "            print(f\"Selected choice(s):\")\n",
        "            for answer in output.answers:\n",
        "                print(f\"Choice: {answer.choice}, Reason: {answer.reason}\")\n",
        "\n",
        "        return ChooseQueryEngineEvent(answers=output, query_str=query_str)\n",
        "\n",
        "    @step()\n",
        "    async def query_each_engine(self, ev: ChooseQueryEngineEvent) -> SynthesizeAnswersEvent:\n",
        "        \"\"\"Query each engine.\"\"\"\n",
        "\n",
        "        query_str = ev.query_str\n",
        "        answers = ev.answers\n",
        "\n",
        "        # query using corresponding query engine given in Answers list\n",
        "        responses = []\n",
        "\n",
        "        for answer in answers.answers:\n",
        "            choice_idx = answer.choice - 1\n",
        "            response = await self._query(query_str, choice_idx)\n",
        "            responses.append(response)\n",
        "\n",
        "        return SynthesizeAnswersEvent(responses=responses, query_str=query_str)\n",
        "\n",
        "    @step()\n",
        "    async def synthesize_response(self, ev: SynthesizeAnswersEvent) -> StopEvent:\n",
        "        \"\"\"Synthesizes response.\"\"\"\n",
        "\n",
        "        responses = ev.responses\n",
        "        query_str = ev.query_str\n",
        "\n",
        "        # get result of responses\n",
        "        if len(responses) == 1:\n",
        "            return StopEvent(result=responses[0])\n",
        "        else:\n",
        "            response_strs = [str(r) for r in responses]\n",
        "            result_response = self.summarizer.get_response(query_str, response_strs)\n",
        "            return StopEvent(result=result_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9SWkq3KxLa-"
      },
      "source": [
        "## Define LlamaCloud Retriever over documents\n",
        "\n",
        "We'll define an instance of `LLamaCloudIndex`, which will allow us to access the indexed docs stored on LlamaCloud. We define two separate retrievers for this index: a file-level retriever and a chunk-level retriever. We create two query engines from these retrievers.\n",
        "\n",
        "After this, we give a description for what each retriever does to allow the LLM to know which one to pick. We'll define our router workflow based on the two query engines and descriptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61bnOA4HxLa-"
      },
      "outputs": [],
      "source": [
        "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
        "\n",
        "index = LlamaCloudIndex(\n",
        "    name=\"<Your Index Name>\",\n",
        "    project_name=\"<Your Project Name>\",\n",
        "    organization_id=\"<Your Org ID>\",\n",
        "    api_key=\"<Your API Key>\"\n",
        ")\n",
        "\n",
        "llm = OpenAI(\"gpt-4o\")\n",
        "\n",
        "doc_retriever = index.as_retriever(retrieval_mode=\"files_via_content\", files_top_k=1)\n",
        "query_engine_doc = RetrieverQueryEngine.from_args(\n",
        "    doc_retriever, llm=llm, response_mode=\"tree_summarize\"\n",
        ")\n",
        "\n",
        "chunk_retriever = index.as_retriever(retrieval_mode=\"chunks\", rerank_top_n=10)\n",
        "query_engine_chunk = RetrieverQueryEngine.from_args(\n",
        "    chunk_retriever, llm=llm, response_mode=\"tree_summarize\"\n",
        ")\n",
        "\n",
        "DOC_METADATA_EXTRA_STR = \"\"\"\\\n",
        "Each document represents a complete 10K report for a given year (e.g. Apple in 2019).\n",
        "Here's an example of relevant documents:\n",
        "1. apple_2019.pdf\n",
        "2. tesla_2020.pdf\n",
        "\"\"\"\n",
        "\n",
        "TOOL_DOC_DESC = f\"\"\"\\\n",
        "Synthesizes an answer to your question by feeding in an entire relevant document as context. Best used for higher-level summarization options.\n",
        "Do NOT use if answer can be found in a specific chunk of a given document. Use the chunk_query_engine instead for that purpose.\n",
        "\n",
        "Below we give details on the format of each document:\n",
        "{DOC_METADATA_EXTRA_STR}\n",
        "\"\"\"\n",
        "\n",
        "TOOL_CHUNK_DESC = f\"\"\"\\\n",
        "Synthesizes an answer to your question by feeding in a relevant chunk as context. Best used for questions that are more pointed in nature.\n",
        "Do NOT use if the question asks seems to require a general summary of any given document. Use the doc_query_engine instead for that purpose.\n",
        "\n",
        "Below we give details on the format of each document:\n",
        "{DOC_METADATA_EXTRA_STR}\n",
        "\"\"\"\n",
        "\n",
        "router_query_workflow = RouterQueryWorkflow(\n",
        "    query_engines=[query_engine_doc, query_engine_chunk],\n",
        "    choice_descriptions=[TOOL_DOC_DESC, TOOL_CHUNK_DESC],\n",
        "    verbose=True,\n",
        "    llm=llm,\n",
        "    router_prompt=ROUTER_PROMPT,\n",
        "    timeout=60\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKgihw-ExLa_"
      },
      "source": [
        "After defining our router query workflow, we'll create a query engine wrapper around this workflow, and we'll define a query engine tool around this wrapper to pass into an agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTjODxfVxLa_"
      },
      "source": [
        "## Creating an Agent Around the Query Engine\n",
        "\n",
        "We'll create a workflow that acts as an agent around the router query engine. In this workflow, we need four events:\n",
        "1. `GatherToolsEvent`: Gets all tools that need to be called (which is determined by the LLM).\n",
        "2. `ToolCallEvent`: An individual tool call. Multiple of these events will be triggered at the same time.\n",
        "3. `ToolCallEventResult`: Gets result from a tool call.\n",
        "4. `GatherEvent`: Returned from dispatcher that triggers the `ToolCallEvent`.\n",
        "\n",
        "This workflow consists of the following steps:\n",
        "1. `chat()`: Appends the message to the chat history. This chat history is fed into the LLM, along with the given tools, and the LLM determines which tools to call. This returns a `GatherToolsEvent`.\n",
        "2. `dispatch_calls()`: Triggers a `ToolCallEvent` for each tool call given in the `GatherToolsEvent` using `send_event()`. Returns a `GatherEvent` with the number of tool calls.\n",
        "3. `call_tool()`: Calls an individual tool. This step will run multiple times if there is more than one tool call. This step calls the tool and appends the result as a chat message to the chat history. It returns a `ToolCallEventResult` with the result of the tool call.\n",
        "4. `gather()`: Gathers the results from all tool calls using `collect_events()`. Waits for all tool calls to finish, then feeds chat history (following all tool calls) into the LLM. Returns the response from the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GqeDLVHxxLa_"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List\n",
        "\n",
        "from llama_index.core.tools import BaseTool\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.core.llms.llm import ToolSelection\n",
        "from llama_index.core.workflow import Context, Workflow\n",
        "from llama_index.core.base.response.schema import Response\n",
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "\n",
        "class InputEvent(Event):\n",
        "    \"\"\"Input event.\"\"\"\n",
        "\n",
        "class GatherToolsEvent(Event):\n",
        "    \"\"\"Gather Tools Event\"\"\"\n",
        "\n",
        "    tool_calls: Any\n",
        "\n",
        "class ToolCallEvent(Event):\n",
        "    \"\"\"Tool Call event\"\"\"\n",
        "\n",
        "    tool_call: ToolSelection\n",
        "\n",
        "class ToolCallEventResult(Event):\n",
        "    \"\"\"Tool call event result.\"\"\"\n",
        "\n",
        "    msg: ChatMessage\n",
        "\n",
        "\n",
        "class RouterOutputAgentWorkflow(Workflow):\n",
        "    \"\"\"Custom router output agent workflow.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "        rag_workflow: Workflow,\n",
        "        timeout: Optional[float] = 10.0,\n",
        "        disable_validation: bool = False,\n",
        "        verbose: bool = False,\n",
        "        llm: Optional[LLM] = None,\n",
        "        chat_history: Optional[List[ChatMessage]] = None,\n",
        "    ):\n",
        "        \"\"\"Constructor.\"\"\"\n",
        "\n",
        "        super().__init__(timeout=timeout, disable_validation=disable_validation, verbose=verbose)\n",
        "\n",
        "        self.rag_workflow = rag_workflow\n",
        "\n",
        "        def query_workflow(query_str: str) -> Response:\n",
        "            \"\"\"Queries 10k reports for a given year.\"\"\"\n",
        "            return self.rag_workflow.run(query_str=query_str)\n",
        "\n",
        "        self.rag_workflow_tool = FunctionTool.from_defaults(query_workflow)\n",
        "\n",
        "        self.llm: LLM = llm or OpenAI(temperature=0, model=\"gpt-4o\")\n",
        "        self.chat_history: List[ChatMessage] = chat_history or []\n",
        "\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        \"\"\"Resets Chat History\"\"\"\n",
        "\n",
        "        self.chat_history = []\n",
        "\n",
        "    @step()\n",
        "    async def prepare_chat(self, ev: StartEvent) -> InputEvent:\n",
        "        message = ev.get(\"message\")\n",
        "        if message is None:\n",
        "            raise ValueError(\"'message' field is required.\")\n",
        "\n",
        "        # add msg to chat history\n",
        "        chat_history = self.chat_history\n",
        "        chat_history.append(ChatMessage(role=\"user\", content=message))\n",
        "        return InputEvent()\n",
        "\n",
        "    @step()\n",
        "    async def chat(self, ev: InputEvent) -> GatherToolsEvent | StopEvent:\n",
        "        \"\"\"Appends msg to chat history, then gets tool calls.\"\"\"\n",
        "\n",
        "        # Put msg into LLM with tools included\n",
        "        chat_res = await self.llm.achat_with_tools(\n",
        "            [self.rag_workflow_tool],\n",
        "            chat_history=self.chat_history,\n",
        "            verbose=self._verbose,\n",
        "            allow_parallel_tool_calls=True\n",
        "        )\n",
        "        tool_calls = self.llm.get_tool_calls_from_response(chat_res, error_on_no_tool_call=False)\n",
        "\n",
        "        ai_message = chat_res.message\n",
        "        self.chat_history.append(ai_message)\n",
        "        if self._verbose:\n",
        "            print(f\"Chat message: {ai_message.content}\")\n",
        "\n",
        "        # no tool calls, return chat message.\n",
        "        if not tool_calls:\n",
        "            return StopEvent(result=ai_message.content)\n",
        "\n",
        "        return GatherToolsEvent(tool_calls=tool_calls)\n",
        "\n",
        "    @step(pass_context=True)\n",
        "    async def dispatch_calls(self, ctx: Context, ev: GatherToolsEvent) -> ToolCallEvent:\n",
        "        \"\"\"Dispatches calls.\"\"\"\n",
        "\n",
        "        tool_calls = ev.tool_calls\n",
        "        await ctx.set(\"num_tool_calls\", len(tool_calls))\n",
        "\n",
        "        # trigger tool call events\n",
        "        for tool_call in tool_calls:\n",
        "            ctx.send_event(ToolCallEvent(tool_call=tool_call))\n",
        "\n",
        "        return None\n",
        "\n",
        "    @step()\n",
        "    async def call_tool(self, ev: ToolCallEvent) -> ToolCallEventResult:\n",
        "        \"\"\"Calls tool.\"\"\"\n",
        "\n",
        "        tool_call = ev.tool_call\n",
        "\n",
        "        # get tool ID and function call\n",
        "        id_ = tool_call.tool_id\n",
        "\n",
        "        if self._verbose:\n",
        "            print(f\"Calling function {tool_call.tool_name} with msg {tool_call.tool_kwargs}\")\n",
        "\n",
        "        # directly run workflow, don't call tools\n",
        "        output = await self.rag_workflow.run(**tool_call.tool_kwargs)\n",
        "        msg = ChatMessage(\n",
        "            name=tool_call.tool_name,\n",
        "            content=str(output),\n",
        "            role=\"tool\",\n",
        "            additional_kwargs={\n",
        "                \"tool_call_id\": id_,\n",
        "                \"name\": tool_call.tool_name\n",
        "            }\n",
        "        )\n",
        "\n",
        "        return ToolCallEventResult(msg=msg)\n",
        "\n",
        "    @step(pass_context=True)\n",
        "    async def gather(self, ctx: Context, ev: ToolCallEventResult) -> StopEvent | None:\n",
        "        \"\"\"Gathers tool calls.\"\"\"\n",
        "        # wait for all tool call events to finish.\n",
        "        tool_events = ctx.collect_events(ev, [ToolCallEventResult] * await ctx.get(\"num_tool_calls\"))\n",
        "        if not tool_events:\n",
        "            return None\n",
        "\n",
        "        for tool_event in tool_events:\n",
        "            # append tool call chat messages to history\n",
        "            self.chat_history.append(tool_event.msg)\n",
        "\n",
        "        # # after all tool calls finish, pass input event back, restart agent loop\n",
        "        return InputEvent()\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBLb-BThxLa_"
      },
      "source": [
        "Creates an instance of the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NLpmFSGhxLa_",
        "outputId": "6c2914fd-87c2-4b5c-d65b-cc86aeb8397b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'router_query_workflow' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a3fbbc01ed7a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRouterOutputAgentWorkflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouter_query_workflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'router_query_workflow' is not defined"
          ]
        }
      ],
      "source": [
        "agent = RouterOutputAgentWorkflow(router_query_workflow, verbose=True, timeout=60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyI64IzAxLa_"
      },
      "source": [
        "#### Visualize Workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kBx6_fbxLa_",
        "outputId": "75f396ed-6f2d-49b8-8249-7375be152610"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "workflow_all_flows.html\n"
          ]
        }
      ],
      "source": [
        "from llama_index.utils.workflow import draw_all_possible_flows\n",
        "\n",
        "draw_all_possible_flows(RouterOutputAgentWorkflow)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ukbDAMLhJACJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmJNL609xLa_"
      },
      "source": [
        "## Example Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uayjWCvnxLbA",
        "outputId": "a173578a-4e8b-4848-bd59-4f14c6960c67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running step prepare_chat\n",
            "Step prepare_chat produced event InputEvent\n",
            "Running step chat\n",
            "Chat message: None\n",
            "Step chat produced event GatherToolsEvent\n",
            "Running step dispatch_calls\n",
            "Step dispatch_calls produced no event\n",
            "Running step call_tool\n",
            "Calling function query_workflow with msg {'query_str': 'Apple revenue 2021'}\n",
            "Running step choose_query_engine\n",
            "Selected choice(s):\n",
            "Choice: 2, Reason: The question 'Apple revenue 2021' is pointed in nature and likely requires specific information from a particular section of the document. Therefore, using a chunk as context is more appropriate.\n",
            "Step choose_query_engine produced event ChooseQueryEngineEvent\n",
            "Running step query_each_engine\n",
            "Step query_each_engine produced event SynthesizeAnswersEvent\n",
            "Running step synthesize_response\n",
            "Step synthesize_response produced event StopEvent\n",
            "Step call_tool produced event ToolCallEventResult\n",
            "Running step call_tool\n",
            "Calling function query_workflow with msg {'query_str': 'Tesla revenue 2021'}\n",
            "Running step gather\n",
            "Step gather produced no event\n",
            "Running step choose_query_engine\n",
            "Selected choice(s):\n",
            "Choice: 2, Reason: The question 'Tesla revenue 2021' is pointed in nature and likely requires specific information from a particular section of the document. Therefore, using a relevant chunk as context is more appropriate.\n",
            "Step choose_query_engine produced event ChooseQueryEngineEvent\n",
            "Running step query_each_engine\n",
            "Step query_each_engine produced event SynthesizeAnswersEvent\n",
            "Running step synthesize_response\n",
            "Step synthesize_response produced event StopEvent\n",
            "Step call_tool produced event ToolCallEventResult\n",
            "Running step gather\n",
            "Step gather produced event InputEvent\n",
            "Running step chat\n",
            "Chat message: In 2021:\n",
            "- **Apple** had a total net sales revenue of **$365.817 billion**.\n",
            "- **Tesla** had a total revenue of **$53.82 billion**.\n",
            "Step chat produced event StopEvent\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "In 2021:\n",
              "- **Apple** had a total net sales revenue of **$365.817 billion**.\n",
              "- **Tesla** had a total revenue of **$53.82 billion**."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "response = await agent.run(message=\"Tell me the revenue for Apple and Tesla in 2021.\")\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pblRASxkxLbA",
        "outputId": "f1a53863-357a-4222-e82b-901eed00fefa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running step prepare_chat\n",
            "Step prepare_chat produced event InputEvent\n",
            "Running step chat\n",
            "Chat message: None\n",
            "Step chat produced event GatherToolsEvent\n",
            "Running step dispatch_calls\n",
            "Step dispatch_calls produced no event\n",
            "Running step call_tool\n",
            "Calling function query_workflow with msg {'query_str': 'Apple tailwinds 2021'}\n",
            "Running step choose_query_engine\n",
            "Selected choice(s):\n",
            "Choice: 2, Reason: The question 'Apple tailwinds 2021' is pointed in nature and likely requires specific information from a relevant chunk of the document rather than a general summary.\n",
            "Step choose_query_engine produced event ChooseQueryEngineEvent\n",
            "Running step query_each_engine\n",
            "Step query_each_engine produced event SynthesizeAnswersEvent\n",
            "Running step synthesize_response\n",
            "Step synthesize_response produced event StopEvent\n",
            "Step call_tool produced event ToolCallEventResult\n",
            "Running step call_tool\n",
            "Calling function query_workflow with msg {'query_str': 'Tesla tailwinds 2021'}\n",
            "Running step gather\n",
            "Step gather produced no event\n",
            "Running step choose_query_engine\n",
            "Selected choice(s):\n",
            "Choice: 2, Reason: The question 'Tesla tailwinds 2021' is pointed in nature and likely requires specific information from a relevant chunk of the document. Choice 2 is best suited for this purpose as it synthesizes an answer by feeding in a relevant chunk as context.\n",
            "Step choose_query_engine produced event ChooseQueryEngineEvent\n",
            "Running step query_each_engine\n",
            "Step query_each_engine produced event SynthesizeAnswersEvent\n",
            "Running step synthesize_response\n",
            "Step synthesize_response produced event StopEvent\n",
            "Step call_tool produced event ToolCallEventResult\n",
            "Running step gather\n",
            "Step gather produced event InputEvent\n",
            "Running step chat\n",
            "Chat message: In 2021, both Apple and Tesla experienced several tailwinds that positively impacted their businesses:\n",
            "\n",
            "### Apple\n",
            "1. **Lower Effective Tax Rate**: Benefits from share-based compensation and foreign-derived intangible income deductions.\n",
            "2. **Deferred Tax Assets**: Significant increases in deferred tax assets for foreign tax credit carryforwards in Ireland and R&D tax credit carryforwards in California.\n",
            "3. **Strong Liquidity Position**: Cash, cash equivalents, and unrestricted marketable securities totaling $172.6 billion.\n",
            "4. **Risk Management**: Effective management of interest rate and foreign currency risks through interest rate swaps and foreign currency swaps.\n",
            "\n",
            "### Tesla\n",
            "1. **Production Ramp-Up**: Increased production rates for updated versions of Model S and Model X, as well as Model 3 and Model Y.\n",
            "2. **Gigafactory Construction**: Progress in the construction of Gigafactory Berlin and Gigafactory Texas, crucial for future production growth.\n",
            "3. **Cost Reduction and Affordability**: Focus on reducing production costs and increasing vehicle affordability.\n",
            "4. **Battery Cell Development**: Continued development and manufacturing of its own battery cells for high-volume output and lower production costs.\n",
            "5. **Market Demand**: Ongoing electrification of the automotive sector and increasing environmental awareness supported demand for Tesla's vehicles.\n",
            "6. **Localized Procurement and Manufacturing**: Efforts in cost reduction and localized procurement and manufacturing allowed competitive pricing, particularly in China.\n",
            "Step chat produced event StopEvent\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "In 2021, both Apple and Tesla experienced several tailwinds that positively impacted their businesses:\n",
              "\n",
              "### Apple\n",
              "1. **Lower Effective Tax Rate**: Benefits from share-based compensation and foreign-derived intangible income deductions.\n",
              "2. **Deferred Tax Assets**: Significant increases in deferred tax assets for foreign tax credit carryforwards in Ireland and R&D tax credit carryforwards in California.\n",
              "3. **Strong Liquidity Position**: Cash, cash equivalents, and unrestricted marketable securities totaling $172.6 billion.\n",
              "4. **Risk Management**: Effective management of interest rate and foreign currency risks through interest rate swaps and foreign currency swaps.\n",
              "\n",
              "### Tesla\n",
              "1. **Production Ramp-Up**: Increased production rates for updated versions of Model S and Model X, as well as Model 3 and Model Y.\n",
              "2. **Gigafactory Construction**: Progress in the construction of Gigafactory Berlin and Gigafactory Texas, crucial for future production growth.\n",
              "3. **Cost Reduction and Affordability**: Focus on reducing production costs and increasing vehicle affordability.\n",
              "4. **Battery Cell Development**: Continued development and manufacturing of its own battery cells for high-volume output and lower production costs.\n",
              "5. **Market Demand**: Ongoing electrification of the automotive sector and increasing environmental awareness supported demand for Tesla's vehicles.\n",
              "6. **Localized Procurement and Manufacturing**: Efforts in cost reduction and localized procurement and manufacturing allowed competitive pricing, particularly in China."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = await agent.run(message=\"Tell me the tailwinds for Apple and Tesla in 2021.\")\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tw8YrcHIxLbA",
        "outputId": "87a7e717-a4f1-4ec9-b6c8-ac25c70654e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running step prepare_chat\n",
            "Step prepare_chat produced event InputEvent\n",
            "Running step chat\n",
            "Chat message: None\n",
            "Step chat produced event GatherToolsEvent\n",
            "Running step dispatch_calls\n",
            "Step dispatch_calls produced no event\n",
            "Running step call_tool\n",
            "Calling function query_workflow with msg {'query_str': 'Apple performance 2019'}\n",
            "Running step choose_query_engine\n",
            "Selected choice(s):\n",
            "Choice: 1, Reason: The question 'Apple performance 2019' requires a general summary of the entire document (Apple's 10K report for 2019). This choice is best suited for higher-level summarization.\n",
            "Step choose_query_engine produced event ChooseQueryEngineEvent\n",
            "Running step query_each_engine\n",
            "Step query_each_engine produced event SynthesizeAnswersEvent\n",
            "Running step synthesize_response\n",
            "Step synthesize_response produced event StopEvent\n",
            "Step call_tool produced event ToolCallEventResult\n",
            "Running step gather\n",
            "Step gather produced event InputEvent\n",
            "Running step chat\n",
            "Chat message: In 2019, Apple Inc. experienced a mixed performance:\n",
            "\n",
            "### Financial Performance\n",
            "- **Total Net Sales**: $260.17 billion, a slight decrease from $265.60 billion in 2018.\n",
            "- **Net Income**: $55.26 billion, down from $59.53 billion in 2018.\n",
            "- **Earnings Per Share**: \n",
            "  - Basic: $2.99 (down from $3.00 in 2018)\n",
            "  - Diluted: $2.97 (down from $2.98 in 2018)\n",
            "\n",
            "### Product and Service Categories\n",
            "- **iPhone Sales**: Declined by 14% to $142.38 billion from $164.89 billion in 2018.\n",
            "- **Other Product Categories**: \n",
            "  - **Mac**: Increased sales.\n",
            "  - **iPad**: Increased sales.\n",
            "  - **Wearables, Home and Accessories**: Increased sales.\n",
            "- **Services**: Significant increase, with net sales rising by 16% to $46.29 billion from $39.75 billion in 2018.\n",
            "\n",
            "### Summary\n",
            "While Apple saw a decline in iPhone sales and overall net income, the company experienced growth in other product categories and services, which helped to offset some of the declines. The services segment, in particular, showed strong growth, indicating a successful diversification of revenue streams.\n",
            "Step chat produced event StopEvent\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "In 2019, Apple Inc. experienced a mixed performance:\n",
              "\n",
              "### Financial Performance\n",
              "- **Total Net Sales**: $260.17 billion, a slight decrease from $265.60 billion in 2018.\n",
              "- **Net Income**: $55.26 billion, down from $59.53 billion in 2018.\n",
              "- **Earnings Per Share**: \n",
              "  - Basic: $2.99 (down from $3.00 in 2018)\n",
              "  - Diluted: $2.97 (down from $2.98 in 2018)\n",
              "\n",
              "### Product and Service Categories\n",
              "- **iPhone Sales**: Declined by 14% to $142.38 billion from $164.89 billion in 2018.\n",
              "- **Other Product Categories**: \n",
              "  - **Mac**: Increased sales.\n",
              "  - **iPad**: Increased sales.\n",
              "  - **Wearables, Home and Accessories**: Increased sales.\n",
              "- **Services**: Significant increase, with net sales rising by 16% to $46.29 billion from $39.75 billion in 2018.\n",
              "\n",
              "### Summary\n",
              "While Apple saw a decline in iPhone sales and overall net income, the company experienced growth in other product categories and services, which helped to offset some of the declines. The services segment, in particular, showed strong growth, indicating a successful diversification of revenue streams."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = await agent.run(message=\"How was apple doing generally in 2019?\")\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG9iPXmExLbA"
      },
      "source": [
        "## [Advanced] Setup Auto-Retrieval for Files\n",
        "\n",
        "We make our file-level retrieval more sophisticated by allowing the LLM to infer a set of metadata filters, based on some relevant example documents. This allows document-level retrieval to be more precise, since it allows the LLM to narrow down search results via metadata filters and not just top-k.\n",
        "\n",
        "We do some advanced things to make this happen\n",
        "- Define a custom prompt to generate metadata filters\n",
        "- Dynamically include few-shot examples of metadata as context to infer the set of metadata filters. These initial few-shot examples of metadata are obtained through vector search.\n",
        "\n",
        "We prompt the LLM to generate a prompt, a list of filters, and optionally a top-k value. We will define another workflow that is subclassed from the `RouterQueryWorkflow`. In this workflow, we will replace the `_query()` method defined in `RouterQueryWorkflow`.\n",
        "\n",
        "In this `_query()` method, we will check if the choice is the document-level retrieval. If it is, then we'll create a new query engine with certain LLM-generated filters applied. We'll return the response from this query engine.\n",
        "\n",
        "A lot of the code below is lifted from our **VectorIndexAutoRetriever** module, which provides an out of the box way to do auto-retrieval against a vector index.\n",
        "\n",
        "Since we are adding some customizations like adding few-shot examples, we re-use prompt pieces and implement auto-retrieval from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fy1-NmXQxLbA"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.prompts import ChatPromptTemplate\n",
        "from llama_index.core.vector_stores.types import (\n",
        "    VectorStoreInfo,\n",
        "    VectorStoreQuerySpec,\n",
        "    MetadataInfo,\n",
        "    MetadataFilters,\n",
        ")\n",
        "\n",
        "SYS_PROMPT = \"\"\"\\\n",
        "Your goal is to structure the user's query to match the request schema provided below.\n",
        "\n",
        "<< Structured Request Schema >>\n",
        "When responding use a markdown code snippet with a JSON object formatted in the \\\n",
        "following schema:\n",
        "\n",
        "{schema_str}\n",
        "\n",
        "The query string should contain only text that is expected to match the contents of \\\n",
        "documents. Any conditions in the filter should not be mentioned in the query as well.\n",
        "\n",
        "Make sure that filters only refer to attributes that exist in the data source.\n",
        "Make sure that filters take into account the descriptions of attributes.\n",
        "Make sure that filters are only used as needed. If there are no filters that should be \\\n",
        "applied return [] for the filter value.\\\n",
        "\n",
        "If the user's query explicitly mentions number of documents to retrieve, set top_k to \\\n",
        "that number, otherwise do not set top_k.\n",
        "\n",
        "The schema of the metadata filters in the vector db table is listed below, along with some example metadata dictionaries from relevant rows.\n",
        "The user will send the input query string.\n",
        "\n",
        "Data Source:\n",
        "```json\n",
        "{info_str}\n",
        "```\n",
        "\n",
        "Example metadata from relevant chunks:\n",
        "{example_rows}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class AutoRetrievalRouterQueryWorkflow(RouterQueryWorkflow):\n",
        "    \"\"\"Router query engine with auto retrieval.\"\"\"\n",
        "\n",
        "    async def _get_auto_retriever_query_engine(\n",
        "        self, query: str, verbose: bool = False\n",
        "    ) -> RetrieverQueryEngine:\n",
        "        \"\"\"Gets auto doc retriever query engine\"\"\"\n",
        "\n",
        "        # retriever that retrieves example rows\n",
        "        example_rows_retriever = index.as_retriever(\n",
        "            retrieval_mode=\"chunks\", rerank_top_n=4\n",
        "        )\n",
        "\n",
        "        def get_example_rows_fn(**kwargs):\n",
        "            \"\"\"Retrieve relevant few-shot examples of metadata.\"\"\"\n",
        "\n",
        "            query_str = kwargs[\"query_str\"]\n",
        "            nodes = example_rows_retriever.retrieve(query_str)\n",
        "            # get the metadata, join them\n",
        "            metadata_list = [n.metadata for n in nodes]\n",
        "\n",
        "            return \"\\n\".join([json.dumps(m) for m in metadata_list])\n",
        "\n",
        "        # define chat prompt template to feed into LLM\n",
        "        chat_prompt_tmpl = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\"system\", SYS_PROMPT),\n",
        "                (\"user\", \"{query_str}\"),\n",
        "            ],\n",
        "            function_mappings={\"example_rows\": get_example_rows_fn},\n",
        "        )\n",
        "\n",
        "        # information about vector store - used to generate json schema in prompt template\n",
        "        vector_store_info = VectorStoreInfo(\n",
        "            content_info=\"document chunks around Apple and Tesla 10K documents\",\n",
        "            metadata_info=[\n",
        "                MetadataInfo(\n",
        "                    name=\"file_name\",\n",
        "                    type=\"str\",\n",
        "                    description=\"Name of the source file\",\n",
        "                ),\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        query_spec: VectorStoreQuerySpec = await llm.astructured_predict(\n",
        "            VectorStoreQuerySpec,\n",
        "            chat_prompt_tmpl,\n",
        "            info_str=vector_store_info.json(indent=4),\n",
        "            schema_str=VectorStoreQuerySpec.schema_json(indent=4),\n",
        "            query_str=query,\n",
        "        )\n",
        "\n",
        "        # build retriever and query engine\n",
        "        filters = (\n",
        "            MetadataFilters(filters=query_spec.filters)\n",
        "            if len(query_spec.filters) > 0\n",
        "            else None\n",
        "        )\n",
        "        if verbose:\n",
        "            print(f\"> Using query str: {query_spec.query}\")\n",
        "\n",
        "        if filters and verbose:\n",
        "            print(f\"> Using filters{filters.json()}\")\n",
        "\n",
        "        retriever = index.as_retriever(\n",
        "            retrieval_mode=\"files_via_content\", files_top_k=1, filters=filters\n",
        "        )\n",
        "\n",
        "        query_engine = RetrieverQueryEngine.from_args(\n",
        "            retriever, llm=self.llm, response_mode=\"tree_summarize\"\n",
        "        )\n",
        "\n",
        "        return query_engine\n",
        "\n",
        "    async def _query(self, query_str: str, choice_idx: int):\n",
        "        \"\"\"Query with auto retriever\"\"\"\n",
        "\n",
        "        if choice_idx == 0:\n",
        "            query_engine = await self._get_auto_retriever_query_engine(\n",
        "                query_str, self._verbose\n",
        "            )\n",
        "        else:\n",
        "            query_engine = self.query_engines[choice_idx]\n",
        "        return await query_engine.aquery(query_str)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EdvRHgKxLbA"
      },
      "source": [
        "Create the auto retrieval query workflow, then wrap it around a RouterQueryEngine, then create a tool around that engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EV5fd7BmxLbA"
      },
      "outputs": [],
      "source": [
        "# auto retrieval query engine\n",
        "auto_retrieval_query_workflow = AutoRetrievalRouterQueryWorkflow(\n",
        "    query_engines=[query_engine_doc, query_engine_chunk],\n",
        "    choice_descriptions=[TOOL_DOC_DESC, TOOL_CHUNK_DESC],\n",
        "    verbose=True,\n",
        "    llm=llm,\n",
        "    router_prompt=ROUTER_PROMPT,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYwGs4ILxLbA"
      },
      "source": [
        "Create an agent using auto retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtIzslHJxLbA"
      },
      "outputs": [],
      "source": [
        "# agent\n",
        "agent_router_output = RouterOutputAgentWorkflow(auto_retrieval_query_workflow, verbose=True, timeout=120)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OunHg2ovxLbA"
      },
      "source": [
        "## Example Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzLbz_3DxLbA",
        "outputId": "15176e47-e741-4b2b-c3b7-88b1f0d9815c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running step prepare_chat\n",
            "Step prepare_chat produced event InputEvent\n",
            "Running step chat\n",
            "Chat message: None\n",
            "Step chat produced event GatherToolsEvent\n",
            "Running step dispatch_calls\n",
            "Step dispatch_calls produced no event\n",
            "Running step call_tool\n",
            "Calling function query_workflow with msg {'query_str': 'Tesla performance in 2021'}\n",
            "Running step choose_query_engine\n",
            "Selected choice(s):\n",
            "Choice: 2, Reason: The question is pointed in nature, asking specifically about Tesla's performance in 2021. Therefore, using a relevant chunk as context would be more appropriate.\n",
            "Step choose_query_engine produced event ChooseQueryEngineEvent\n",
            "Running step query_each_engine\n",
            "Step query_each_engine produced event SynthesizeAnswersEvent\n",
            "Running step synthesize_response\n",
            "Step synthesize_response produced event StopEvent\n",
            "Step call_tool produced event ToolCallEventResult\n",
            "Running step call_tool\n",
            "Calling function query_workflow with msg {'query_str': 'Tesla performance in 2022'}\n",
            "Running step gather\n",
            "Step gather produced no event\n",
            "Running step choose_query_engine\n",
            "Selected choice(s):\n",
            "Choice: 2, Reason: The question is pointed in nature, asking specifically about Tesla's performance in 2022. Therefore, using a relevant chunk as context is more appropriate.\n",
            "Step choose_query_engine produced event ChooseQueryEngineEvent\n",
            "Running step query_each_engine\n",
            "Step query_each_engine produced event SynthesizeAnswersEvent\n",
            "Running step synthesize_response\n",
            "Step synthesize_response produced event StopEvent\n",
            "Step call_tool produced event ToolCallEventResult\n",
            "Running step gather\n",
            "Step gather produced event InputEvent\n",
            "Running step chat\n",
            "Chat message: ### Tesla's Performance in 2021\n",
            "\n",
            "- **Total Revenues**: $53.82 billion (up from $31.54 billion in 2020)\n",
            "  - Automotive Segment: $47.23 billion\n",
            "  - Energy Generation and Storage Segment: $2.79 billion\n",
            "- **Gross Profit**: $13.61 billion (up from $6.63 billion in 2020)\n",
            "- **Net Income**: $5.52 billion (up from $721 million in 2020)\n",
            "  - Net Income per Share: $5.60 (basic), $4.90 (diluted)\n",
            "- **Operating Expenses**: $7.08 billion (increased due to higher R&D costs, SG&A expenses, and stock-based compensation)\n",
            "- **Income from Operations**: $6.52 billion (up from $1.99 billion in 2020)\n",
            "\n",
            "Overall, Tesla experienced significant growth in revenue, profitability, and operational efficiency in 2021.\n",
            "\n",
            "### Tesla's Performance in 2022\n",
            "\n",
            "- **Total Revenues**: $81.46 billion (up by $27.64 billion from 2021)\n",
            "- **Net Income**: $12.56 billion (up by $7.04 billion from 2021)\n",
            "- **Cash and Cash Equivalents**: $22.19 billion (up by $4.48 billion from the end of 2021)\n",
            "- **Vehicle Production**: 1,369,611 consumer vehicles\n",
            "- **Vehicle Deliveries**: 1,313,851 consumer vehicles\n",
            "- **Energy Storage Products Deployed**: 6.5 GWh\n",
            "- **Solar Energy Systems Deployed**: 348 megawatts\n",
            "\n",
            "In 2022, Tesla continued its growth trajectory with substantial increases in revenue, net income, and cash reserves, along with significant achievements in vehicle production and energy product deployments.\n",
            "Step chat produced event StopEvent\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "### Tesla's Performance in 2021\n",
              "\n",
              "- **Total Revenues**: $53.82 billion (up from $31.54 billion in 2020)\n",
              "  - Automotive Segment: $47.23 billion\n",
              "  - Energy Generation and Storage Segment: $2.79 billion\n",
              "- **Gross Profit**: $13.61 billion (up from $6.63 billion in 2020)\n",
              "- **Net Income**: $5.52 billion (up from $721 million in 2020)\n",
              "  - Net Income per Share: $5.60 (basic), $4.90 (diluted)\n",
              "- **Operating Expenses**: $7.08 billion (increased due to higher R&D costs, SG&A expenses, and stock-based compensation)\n",
              "- **Income from Operations**: $6.52 billion (up from $1.99 billion in 2020)\n",
              "\n",
              "Overall, Tesla experienced significant growth in revenue, profitability, and operational efficiency in 2021.\n",
              "\n",
              "### Tesla's Performance in 2022\n",
              "\n",
              "- **Total Revenues**: $81.46 billion (up by $27.64 billion from 2021)\n",
              "- **Net Income**: $12.56 billion (up by $7.04 billion from 2021)\n",
              "- **Cash and Cash Equivalents**: $22.19 billion (up by $4.48 billion from the end of 2021)\n",
              "- **Vehicle Production**: 1,369,611 consumer vehicles\n",
              "- **Vehicle Deliveries**: 1,313,851 consumer vehicles\n",
              "- **Energy Storage Products Deployed**: 6.5 GWh\n",
              "- **Solar Energy Systems Deployed**: 348 megawatts\n",
              "\n",
              "In 2022, Tesla continued its growth trajectory with substantial increases in revenue, net income, and cash reserves, along with significant achievements in vehicle production and energy product deployments."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = await agent_router_output.run(message=\"How was Tesla doing generally in 2021 and 2022?\")\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY0qlR8uxLbB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llamacloud-demo",
      "language": "python",
      "name": "llamacloud-demo"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}